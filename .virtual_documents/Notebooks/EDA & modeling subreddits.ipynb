#imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import os


#sklearn
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import StandardScaler


#nltk
import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

#Ignore warnings
from warnings import filterwarnings
filterwarnings('ignore')


subreddits = pd.read_csv('../csv_data/master_subreddit.csv')


subreddits.head()


subreddits.columns


subreddits.rename(columns={'posttext': 'selftext'}, inplace=True)


subreddits.dtypes


subreddits.info()





subreddits['selftext'].fillna('title', inplace = True)


subreddits['selftext'].value_counts()
print(f'Self Text Null is: {subreddits["selftext"].isnull().sum()}')


subreddits.isna().sum()


#master_df.duplicated(title).sum()
subreddits.duplicated(subset=['title']).sum()


subreddits.drop_duplicates(subset=['selftext'], inplace=True)

subreddits.duplicated(subset=['selftext']).sum()


subreddits.shape


def preprocess_dataframe(df):
    # Normalize numerical columns
    numerical_cols = ['upvote_ratio', 'num_comments']
    df[numerical_cols] = StandardScaler().fit_transform(df[numerical_cols])

    # Process 'time_posted' column (timestamp)
    df['time_posted'] = pd.to_datetime(df['time_posted'])  # No need to specify unit

    # Extract hour_of_day and day_of_week from 'time_posted'
    df['hour_of_day'] = df['time_posted'].dt.hour
    df['day_of_week'] = df['time_posted'].dt.dayofweek

    # Drop 'time_posted' column after extracting hour_of_day and day_of_week
    df = df.drop('time_posted', axis=1)

    return df


#Checking the cleaned data
processed_subreddits = preprocess_dataframe(subreddits)
print(processed_subreddits.shape)
print(processed_subreddits.tail(10))


'''This function will clean and normalize the text data to make 
it suitable for further analysis and modeling in NLP tasks. '''

def preprocess_text(text):
 
    # Lowercasing the text
    text = text.lower()
    # Removing special characters and digits
    text = re.sub("(\\d|\\W)+", " ", text)
    # Tokenize into words
    words = text.split()
    # Remove stopwords
    stop_words = set(stopwords.words("english", "ascii"))
    words = [word for word in words if word not in stop_words]
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    return " ".join(words)


'''  
The code involves preparing the data through preprocessing steps and dividing it into
training and testing sets. Additionally, it creates a vectorizer object to transform
the text data into numerical representations appropriate for NLP tasks.
'''
def prepare_data_for_nlp(df, text_column='selftext', target_column='subreddit'):
    
    # Preprocess text data
    df[text_column] = df[text_column].apply(lambda x: preprocess_text(str(x)))

    # Split dataset into features and target variable
    X = df[text_column]
    y = df[target_column]

    # Splitting the dataset into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Vectorization - using TF-IDF
    vectorizer = TfidfVectorizer(max_features=5000)
    X_train_vect = vectorizer.fit_transform(X_train)
    X_test_vect = vectorizer.transform(X_test)

    return X_train_vect, X_test_vect, y_train, y_test, vectorizer


#splitting the data into training and testing sets
X_train, X_test, y_train, y_test, vectorizer = prepare_data_for_nlp(processed_subreddits)


X_train


X_test


pd.DataFrame(X_train.toarray(), columns = vectorizer.get_feature_names_out())


#saving it
td_idf_df = pd.DataFrame(X_train.toarray(), columns = vectorizer.get_feature_names_out())


#sum the tdidf score
word_frequencies = td_idf_df.sum(axis = 0).to_dict()


# Convert dictionary to DataFrame
word_freq_df = pd.DataFrame(word_frequencies.items(), columns=['Word', 'Frequency'])

# Sort DataFrame by frequency in descending order
word_freq_df_sorted = word_freq_df.sort_values(by='Frequency', ascending=False)

print(word_freq_df_sorted)


# Select the top 25 rows
top_25_words = word_freq_df_sorted.head(25)

# Plot the top 25 words as a bar plot
plt.figure(figsize=(10, 6))  # Adjust figure size if needed
plt.bar(top_25_words['Word'], top_25_words['Frequency'])
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.title('Top 25 Word Frequencies')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()  # Adjust layout to prevent clipping of labels
plt.show()



